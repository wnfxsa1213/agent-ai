"""
å‘é‡æ•°æ®åº“ç®¡ç†ç•Œé¢ - æä¾›å›¾å½¢åŒ–æ“ä½œå‘é‡æ•°æ®åº“çš„åŠŸèƒ½
"""

import os
import sys
import streamlit as st
import shutil
from datetime import datetime
import json
import zipfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# ç°åœ¨å¯¼å…¥é¡¹ç›®å†…çš„æ¨¡å—
from core.config_manager import ConfigManager

# å¯¼å…¥å¿…è¦çš„åº“
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader, Docx2txtLoader, TextLoader, CSVLoader
)

# è·¯å¾„è®¾ç½®
DEFAULT_DB_PATH = os.path.join(parent_dir, "reference_dbs")
TEMP_DIR = os.path.join(parent_dir, "temp")
CONFIG_PATH = os.path.join(parent_dir, "config.ini")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(DEFAULT_DB_PATH, exist_ok=True)
os.makedirs(TEMP_DIR, exist_ok=True)

# æ–‡æ¡£åŠ è½½å’Œå¤„ç†å‡½æ•°
def load_document(file_path):
    """æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    if file_ext == '.pdf':
        loader = PyPDFLoader(file_path)
    elif file_ext in ['.docx', '.doc']:
        loader = Docx2txtLoader(file_path)
    elif file_ext in ['.csv', '.tsv']:
        loader = CSVLoader(file_path)
    else:  # é»˜è®¤ä½œä¸ºæ–‡æœ¬æ–‡ä»¶å¤„ç†
        loader = TextLoader(file_path, encoding='utf-8')
        
    return loader.load()

def split_documents(documents, chunk_size=1000, chunk_overlap=200):
    """å°†æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„å—"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    return text_splitter.split_documents(documents)

def get_embeddings(embedding_type="openai", api_key=None, model=None, api_base=None):
    """è·å–åµŒå…¥å¯¹è±¡"""
    config = ConfigManager(CONFIG_PATH)
    
    if embedding_type == "openai":
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„APIå¯†é’¥
        if not api_key:
            api_key = config.get("embeddings", "api_key", fallback="")
            if not api_key:
                api_key = os.environ.get("OPENAI_API_KEY", "")
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°
        if not model:
            model = config.get("embeddings", "model", fallback="text-embedding-3-small")
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„APIåŸºç¡€URL
        if not api_base:
            api_base = config.get("embeddings", "api_base", fallback="https://api.openai.com/v1")
        
        return OpenAIEmbeddings(
            openai_api_key=api_key,
            model=model,
            openai_api_base=api_base  # æ·»åŠ è‡ªå®šä¹‰APIåœ°å€
        )
    elif embedding_type == "local":
        model_name = config.get("embeddings", "local_model", 
                              fallback="shibing624/text2vec-base-chinese")
        
        return HuggingFaceEmbeddings(
            model_name=model_name
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„åµŒå…¥ç±»å‹: {embedding_type}")

# ä¸»UIç•Œé¢
def main():
    st.set_page_config(
        page_title="å‘é‡æ•°æ®åº“ç®¡ç†å·¥å…·",
        page_icon="ğŸ“š",
        layout="wide"
    )
    
    # å¯¼å…¥é…ç½®
    config = ConfigManager(CONFIG_PATH)
    
    # æ ‡é¢˜å’Œä»‹ç»
    st.title("ğŸ“š å‘é‡æ•°æ®åº“ç®¡ç†å·¥å…·")
    st.markdown("""
    è¿™ä¸ªå·¥å…·å¯ä»¥å¸®åŠ©æ‚¨å¯è§†åŒ–æ“ä½œå‘é‡æ•°æ®åº“ï¼ŒåŒ…æ‹¬åˆ›å»ºæ•°æ®åº“ã€æ·»åŠ å†…å®¹ã€æœç´¢å’Œç®¡ç†ã€‚
    """)
    
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")
        
        # åµŒå…¥æ¨¡å‹é…ç½®
        embedding_type = st.radio(
            "é€‰æ‹©åµŒå…¥æ¨¡å‹ç±»å‹",
            ["OpenAI (åœ¨çº¿)", "HuggingFace (æœ¬åœ°)"],
            index=0
        )
        
        embedding_provider = "openai" if embedding_type == "OpenAI (åœ¨çº¿)" else "local"
        
        # OpenAIè®¾ç½®
        if embedding_provider == "openai":
            api_key = st.text_input(
                "OpenAI APIå¯†é’¥", 
                value=config.get("embeddings", "api_key", fallback=""),
                type="password"
            )
            
            api_base = st.text_input(
                "OpenAI APIåœ°å€",
                value=config.get("embeddings", "api_base", fallback="https://api.openai.com/v1"),
                help="è‡ªå®šä¹‰APIåœ°å€ï¼Œä¾‹å¦‚ä»£ç†æœåŠ¡å™¨åœ°å€"
            )
            
            model = st.selectbox(
                "åµŒå…¥æ¨¡å‹",
                ["text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"],
                index=0
            )
            
            # ä¿å­˜è®¾ç½®åˆ°é…ç½®æ–‡ä»¶
            if st.button("ä¿å­˜APIè®¾ç½®"):
                if api_key:
                    config.set("embeddings", "provider", "openai")
                    config.set("embeddings", "api_key", api_key)
                    config.set("embeddings", "model", model)
                    config.set("embeddings", "api_base", api_base)  # ä¿å­˜APIåœ°å€
                    config.save()
                    st.success("âœ… è®¾ç½®å·²ä¿å­˜")
                else:
                    st.error("âŒ APIå¯†é’¥ä¸èƒ½ä¸ºç©º")
        
        # æœ¬åœ°æ¨¡å‹è®¾ç½®
        else:
            local_model = st.text_input(
                "æœ¬åœ°æ¨¡å‹åç§°",
                value=config.get("embeddings", "local_model", fallback="shibing624/text2vec-base-chinese")
            )
            
            # ä¿å­˜è®¾ç½®åˆ°é…ç½®æ–‡ä»¶
            if st.button("ä¿å­˜æœ¬åœ°æ¨¡å‹è®¾ç½®"):
                if local_model:
                    config.set("embeddings", "provider", "local")
                    config.set("embeddings", "local_model", local_model)
                    config.save()
                    st.success("âœ… è®¾ç½®å·²ä¿å­˜")
                else:
                    st.error("âŒ æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
        
        st.markdown("---")
        st.markdown("### æ–‡æ¡£è®¾ç½®")
        
        chunk_size = st.slider(
            "æ–‡æœ¬å—å¤§å°",
            min_value=100,
            max_value=2000,
            value=1000,
            step=100,
            help="æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ã€‚è¾ƒå°çš„å—å¯æé«˜æœç´¢å‡†ç¡®åº¦ï¼Œä½†ä¼šå¢åŠ å¤„ç†æ—¶é—´å’Œå­˜å‚¨ç©ºé—´ã€‚"
        )
        
        chunk_overlap = st.slider(
            "æ–‡æœ¬å—é‡å ",
            min_value=0,
            max_value=500,
            value=200,
            step=50,
            help="ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´é‡å çš„å­—ç¬¦æ•°ã€‚å¢åŠ é‡å å¯ä»¥æé«˜è¿ç»­æ€§ï¼Œä½†ä¼šå¢åŠ å­˜å‚¨ç©ºé—´ã€‚"
        )
    
    # ä¸»è¦æ“ä½œåŒºåŸŸ
    tabs = st.tabs(["åˆ›å»ºæ•°æ®åº“", "æ·»åŠ å†…å®¹", "æœç´¢", "ç®¡ç†æ•°æ®åº“"])
    
    # è·å–ç°æœ‰æ•°æ®åº“åˆ—è¡¨
    existing_dbs = []
    if os.path.exists(DEFAULT_DB_PATH):
        existing_dbs = [d for d in os.listdir(DEFAULT_DB_PATH) 
                        if os.path.isdir(os.path.join(DEFAULT_DB_PATH, d))]
    
    # Tab 1: åˆ›å»ºæ•°æ®åº“
    with tabs[0]:
        st.header("ğŸ†• åˆ›å»ºæ–°æ•°æ®åº“")
        
        # æ•°æ®åº“åç§°
        db_name = st.text_input("æ•°æ®åº“åç§°", key="create_db_name")
        
        # è¾“å…¥æ–¹å¼é€‰æ‹©
        input_method = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼", ["ä¸Šä¼ æ–‡ä»¶", "ä¸Šä¼ æ–‡ä»¶å¤¹", "ç›´æ¥è¾“å…¥æ–‡æœ¬"], key="create_input_method")
        
        if input_method == "ä¸Šä¼ æ–‡ä»¶":
            uploaded_file = st.file_uploader(
                "é€‰æ‹©æ–‡ä»¶", 
                type=["pdf", "txt", "docx", "doc", "csv"],
                help="æ”¯æŒPDFã€Wordã€TXTå’ŒCSVæ–‡ä»¶"
            )
            
            if uploaded_file and db_name:
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                file_path = os.path.join(TEMP_DIR, uploaded_file.name)
                
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                st.success(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")
                
                if st.button("å¤„ç†æ–‡ä»¶å¹¶åˆ›å»ºæ•°æ®åº“", key="create_from_file"):
                    with st.spinner("å¤„ç†ä¸­..."):
                        try:
                            # åŠ è½½æ–‡æ¡£
                            documents = load_document(file_path)
                            
                            # åˆ†å‰²æ–‡æ¡£
                            chunks = split_documents(documents, chunk_size, chunk_overlap)
                            
                            if not chunks:
                                st.error("âŒ æ— æ³•ä»æ–‡ä»¶ä¸­æå–å†…å®¹")
                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.remove(file_path)
                                st.stop()
                            
                            # è·å–åµŒå…¥å¯¹è±¡
                            try:
                                embeddings = get_embeddings(
                                    embedding_provider,
                                    api_key if embedding_provider == "openai" else None,
                                    model if embedding_provider == "openai" else None,
                                    api_base if embedding_provider == "openai" else None
                                )
                            except Exception as e:
                                st.error(f"âŒ åˆ›å»ºåµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.remove(file_path)
                                st.stop()
                            
                            # åˆ›å»ºæ•°æ®åº“ç›®å½•
                            db_path = os.path.join(DEFAULT_DB_PATH, db_name)
                            os.makedirs(db_path, exist_ok=True)
                            
                            # åˆ›å»ºå‘é‡æ•°æ®åº“
                            vector_db = Chroma.from_documents(
                                documents=chunks,
                                embedding=embeddings,
                                persist_directory=db_path
                            )
                            vector_db.persist()
                            
                            # ä¿å­˜æ•°æ®åº“å…ƒä¿¡æ¯
                            metadata = {
                                "name": db_name,
                                "file_name": uploaded_file.name,
                                "file_type": os.path.splitext(uploaded_file.name)[1].lstrip('.'),
                                "chunk_size": chunk_size,
                                "chunk_overlap": chunk_overlap,
                                "chunk_count": len(chunks),
                                "embedding_provider": embedding_provider,
                                "embedding_model": model if embedding_provider == "openai" else local_model,
                                "created_at": datetime.now().isoformat(),
                            }
                            
                            with open(os.path.join(db_path, "metadata.json"), "w", encoding="utf-8") as f:
                                json.dump(metadata, f, ensure_ascii=False, indent=2)
                            
                            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                            os.remove(file_path)
                            
                            st.success(f"âœ… æ•°æ®åº“ '{db_name}' åˆ›å»ºæˆåŠŸï¼å…±åŒ…å« {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
                            
                            # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
                            st.info(f"""
                            ğŸ“Š æ•°æ®åº“ä¿¡æ¯:
                            - åç§°: {db_name}
                            - æ¥æºæ–‡ä»¶: {uploaded_file.name}
                            - æ–‡æœ¬å—æ•°é‡: {len(chunks)}
                            - åµŒå…¥æ¨¡å‹: {"OpenAI" if embedding_provider == "openai" else "æœ¬åœ°"} ({model if embedding_provider == "openai" else local_model})
                            - åˆ›å»ºæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                            """)
                        
                        except Exception as e:
                            st.error(f"âŒ åˆ›å»ºæ•°æ®åº“å¤±è´¥: {str(e)}")
                            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                            if os.path.exists(file_path):
                                os.remove(file_path)
        
        elif input_method == "ä¸Šä¼ æ–‡ä»¶å¤¹":
            st.info("ğŸ“ è¯·æŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„ï¼ˆéœ€è¦åœ¨æœåŠ¡å™¨ä¸Šå¯è®¿é—®ï¼‰")
            folder_path = st.text_input("æ–‡ä»¶å¤¹è·¯å¾„", key="create_folder_path")
            
            file_types = st.multiselect(
                "é€‰æ‹©æ–‡ä»¶ç±»å‹",
                ["pdf", "docx", "doc", "txt", "csv"],
                default=["pdf", "docx", "txt"],
                help="é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶ç±»å‹"
            )
            
            recursive = st.checkbox("é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹", value=True)
            
            if folder_path and db_name and file_types:
                if not os.path.exists(folder_path):
                    st.error(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
                else:
                    if st.button("å¤„ç†æ–‡ä»¶å¤¹å¹¶åˆ›å»ºæ•°æ®åº“", key="create_from_folder"):
                        with st.spinner("å¤„ç†ä¸­..."):
                            try:
                                # æ”¶é›†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶
                                all_files = []
                                
                                if recursive:
                                    for root, _, files in os.walk(folder_path):
                                        for file in files:
                                            if any(file.lower().endswith(f".{ft}") for ft in file_types):
                                                all_files.append(os.path.join(root, file))
                                else:
                                    for file in os.listdir(folder_path):
                                        file_path = os.path.join(folder_path, file)
                                        if os.path.isfile(file_path) and any(file.lower().endswith(f".{ft}") for ft in file_types):
                                            all_files.append(file_path)
                                
                                if not all_files:
                                    st.error(f"âŒ åœ¨æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡æ¡£")
                                    st.stop()
                                
                                # åˆ›å»ºè¿›åº¦æ¡
                                progress_bar = st.progress(0)
                                progress_text = st.empty()
                                
                                # åŠ è½½å¹¶å¤„ç†æ‰€æœ‰æ–‡æ¡£
                                documents = []
                                file_stats = {"total": 0}
                                
                                for i, file_path in enumerate(all_files):
                                    progress_text.text(f"å¤„ç†æ–‡ä»¶ {i+1}/{len(all_files)}: {os.path.basename(file_path)}")
                                    progress_bar.progress((i) / len(all_files))
                                    
                                    try:
                                        file_ext = os.path.splitext(file_path)[1].lower().lstrip('.')
                                        file_stats["total"] += 1
                                        file_stats[file_ext] = file_stats.get(file_ext, 0) + 1
                                        
                                        docs = load_document(file_path)
                                        
                                        # æ·»åŠ æ–‡ä»¶åå…ƒæ•°æ®
                                        for doc in docs:
                                            if not hasattr(doc, 'metadata'):
                                                doc.metadata = {}
                                            doc.metadata['source_file'] = os.path.basename(file_path)
                                            
                                        documents.extend(docs)
                                        
                                    except Exception as e:
                                        st.warning(f"âš ï¸ å¤„ç†æ–‡ä»¶ '{os.path.basename(file_path)}' æ—¶å‡ºé”™: {str(e)}")
                                
                                progress_bar.progress(1.0)
                                progress_text.text(f"åˆ†å‰²æ–‡æ¡£...")
                                
                                if not documents:
                                    st.error("âŒ æˆåŠŸå¤„ç†æ–‡ä»¶ï¼Œä½†æœªæå–åˆ°å†…å®¹")
                                    st.stop()
                                    
                                # åˆ†å‰²æ–‡æ¡£
                                chunks = split_documents(documents, chunk_size, chunk_overlap)
                                
                                progress_text.text(f"åˆ›å»ºå‘é‡æ•°æ®åº“...")
                                
                                # è·å–åµŒå…¥å¯¹è±¡
                                try:
                                    embeddings = get_embeddings(
                                        embedding_provider,
                                        api_key if embedding_provider == "openai" else None,
                                        model if embedding_provider == "openai" else None,
                                        api_base if embedding_provider == "openai" else None
                                    )
                                except Exception as e:
                                    st.error(f"âŒ åˆ›å»ºåµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
                                    st.stop()
                                
                                # åˆ›å»ºæ•°æ®åº“ç›®å½•
                                db_path = os.path.join(DEFAULT_DB_PATH, db_name)
                                os.makedirs(db_path, exist_ok=True)
                                
                                # åˆ›å»ºå‘é‡æ•°æ®åº“
                                vector_db = Chroma.from_documents(
                                    documents=chunks,
                                    embedding=embeddings,
                                    persist_directory=db_path
                                )
                                vector_db.persist()
                                
                                # ç”Ÿæˆæ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
                                file_report = ", ".join([f"{ext}: {count}" for ext, count in file_stats.items() if ext != "total"])
                                
                                # ä¿å­˜æ•°æ®åº“å…ƒä¿¡æ¯
                                metadata = {
                                    "name": db_name,
                                    "directory": os.path.abspath(folder_path),
                                    "file_count": file_stats,
                                    "chunk_size": chunk_size,
                                    "chunk_overlap": chunk_overlap,
                                    "chunk_count": len(chunks),
                                    "embedding_provider": embedding_provider,
                                    "embedding_model": model if embedding_provider == "openai" else local_model,
                                    "created_at": datetime.now().isoformat(),
                                }
                                
                                with open(os.path.join(db_path, "metadata.json"), "w", encoding="utf-8") as f:
                                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                                
                                # æ›´æ–°è¿›åº¦å¹¶æ˜¾ç¤ºå®Œæˆä¿¡æ¯
                                progress_text.empty()
                                progress_bar.empty()
                                
                                st.success(f"âœ… æ•°æ®åº“ '{db_name}' åˆ›å»ºæˆåŠŸï¼å…±å¤„ç† {file_stats['total']} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
                                
                                # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
                                st.info(f"""
                                ğŸ“Š æ•°æ®åº“ä¿¡æ¯:
                                - åç§°: {db_name}
                                - æ¥æºç›®å½•: {folder_path}
                                - å¤„ç†æ–‡ä»¶: {file_stats['total']} ä¸ª ({file_report})
                                - æ–‡æœ¬å—æ•°é‡: {len(chunks)}
                                - åµŒå…¥æ¨¡å‹: {"OpenAI" if embedding_provider == "openai" else "æœ¬åœ°"} ({model if embedding_provider == "openai" else local_model})
                                - åˆ›å»ºæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                                """)
                            
                            except Exception as e:
                                st.error(f"âŒ åˆ›å»ºæ•°æ®åº“å¤±è´¥: {str(e)}")
        
        elif input_method == "ç›´æ¥è¾“å…¥æ–‡æœ¬":
            text_input = st.text_area(
                "è¾“å…¥æ–‡æœ¬å†…å®¹", 
                height=300,
                help="ç›´æ¥è¾“å…¥æˆ–ç²˜è´´æ–‡æœ¬å†…å®¹"
            )
            
            if text_input and db_name:
                if st.button("å¤„ç†æ–‡æœ¬å¹¶åˆ›å»ºæ•°æ®åº“", key="create_from_text"):
                    with st.spinner("å¤„ç†ä¸­..."):
                        try:
                            # åˆ†å‰²æ–‡æœ¬
                            text_splitter = RecursiveCharacterTextSplitter(
                                chunk_size=chunk_size,
                                chunk_overlap=chunk_overlap
                            )
                            chunks = text_splitter.create_documents([text_input])
                            
                            # è·å–åµŒå…¥å¯¹è±¡
                            try:
                                embeddings = get_embeddings(
                                    embedding_provider,
                                    api_key if embedding_provider == "openai" else None,
                                    model if embedding_provider == "openai" else None,
                                    api_base if embedding_provider == "openai" else None
                                )
                            except Exception as e:
                                st.error(f"âŒ åˆ›å»ºåµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
                                st.stop()
                            
                            # åˆ›å»ºæ•°æ®åº“ç›®å½•
                            db_path = os.path.join(DEFAULT_DB_PATH, db_name)
                            os.makedirs(db_path, exist_ok=True)
                            
                            # åˆ›å»ºå‘é‡æ•°æ®åº“
                            vector_db = Chroma.from_documents(
                                documents=chunks,
                                embedding=embeddings,
                                persist_directory=db_path
                            )
                            vector_db.persist()
                            
                            # ä¿å­˜æ•°æ®åº“å…ƒä¿¡æ¯
                            metadata = {
                                "name": db_name,
                                "source": "ç›´æ¥è¾“å…¥æ–‡æœ¬",
                                "text_length": len(text_input),
                                "chunk_size": chunk_size,
                                "chunk_overlap": chunk_overlap,
                                "chunk_count": len(chunks),
                                "embedding_provider": embedding_provider,
                                "embedding_model": model if embedding_provider == "openai" else local_model,
                                "created_at": datetime.now().isoformat(),
                            }
                            
                            with open(os.path.join(db_path, "metadata.json"), "w", encoding="utf-8") as f:
                                json.dump(metadata, f, ensure_ascii=False, indent=2)
                            
                            st.success(f"âœ… æ•°æ®åº“ '{db_name}' åˆ›å»ºæˆåŠŸï¼å…±åŒ…å« {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
                            
                            # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
                            st.info(f"""
                            ğŸ“Š æ•°æ®åº“ä¿¡æ¯:
                            - åç§°: {db_name}
                            - æ¥æº: ç›´æ¥è¾“å…¥æ–‡æœ¬
                            - æ–‡æœ¬é•¿åº¦: {len(text_input)} å­—ç¬¦
                            - æ–‡æœ¬å—æ•°é‡: {len(chunks)}
                            - åµŒå…¥æ¨¡å‹: {"OpenAI" if embedding_provider == "openai" else "æœ¬åœ°"} ({model if embedding_provider == "openai" else local_model})
                            - åˆ›å»ºæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                            """)
                            
                        except Exception as e:
                            st.error(f"âŒ åˆ›å»ºæ•°æ®åº“å¤±è´¥: {str(e)}")
    
    # Tab 2: æ·»åŠ å†…å®¹
    with tabs[1]:
        st.header("â• å‘æ•°æ®åº“æ·»åŠ å†…å®¹")
        
        if not existing_dbs:
            st.info("âš ï¸ ç›®å‰æ²¡æœ‰ç°æœ‰æ•°æ®åº“ï¼Œè¯·å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®åº“")
        else:
            selected_db = st.selectbox("é€‰æ‹©æ•°æ®åº“", existing_dbs, key="add_db_select")
            
            # è¾“å…¥æ–¹å¼é€‰æ‹©
            input_method = st.radio("é€‰æ‹©è¾“å…¥æ–¹å¼", ["ä¸Šä¼ æ–‡ä»¶", "ç›´æ¥è¾“å…¥æ–‡æœ¬"], key="add_input_method")
            
            if input_method == "ä¸Šä¼ æ–‡ä»¶":
                uploaded_file = st.file_uploader(
                    "é€‰æ‹©æ–‡ä»¶", 
                    type=["pdf", "txt", "docx", "doc", "csv"],
                    key="add_file_uploader"
                )
                
                if uploaded_file and selected_db:
                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    file_path = os.path.join(TEMP_DIR, uploaded_file.name)
                    
                    with open(file_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                    
                    st.success(f"âœ… æ–‡ä»¶å·²ä¸Šä¼ : {uploaded_file.name}")
                    
                    if st.button("å¤„ç†æ–‡ä»¶å¹¶æ·»åŠ åˆ°æ•°æ®åº“", key="add_from_file"):
                        with st.spinner("å¤„ç†ä¸­..."):
                            try:
                                # åŠ è½½æ–‡æ¡£
                                documents = load_document(file_path)
                                
                                # åˆ†å‰²æ–‡æ¡£
                                chunks = split_documents(documents, chunk_size, chunk_overlap)
                                
                                if not chunks:
                                    st.error("âŒ æ— æ³•ä»æ–‡ä»¶ä¸­æå–å†…å®¹")
                                    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                    os.remove(file_path)
                                    st.stop()
                                
                                # è·å–åµŒå…¥å¯¹è±¡
                                try:
                                    embeddings = get_embeddings(
                                        embedding_provider,
                                        api_key if embedding_provider == "openai" else None,
                                        model if embedding_provider == "openai" else None,
                                        api_base if embedding_provider == "openai" else None
                                    )
                                except Exception as e:
                                    st.error(f"âŒ åˆ›å»ºåµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
                                    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                    os.remove(file_path)
                                    st.stop()
                                
                                # åŠ è½½ç°æœ‰æ•°æ®åº“
                                db_path = os.path.join(DEFAULT_DB_PATH, selected_db)
                                
                                vector_db = Chroma(
                                    persist_directory=db_path,
                                    embedding_function=embeddings
                                )
                                
                                # æ·»åŠ æ–°æ–‡æ¡£
                                vector_db.add_documents(chunks)
                                vector_db.persist()
                                
                                # æ›´æ–°å…ƒæ•°æ®
                                metadata_path = os.path.join(db_path, "metadata.json")
                                if os.path.exists(metadata_path):
                                    try:
                                        with open(metadata_path, "r", encoding="utf-8") as f:
                                            metadata = json.load(f)
                                            
                                        # æ›´æ–°å…ƒæ•°æ®
                                        metadata["chunk_count"] = metadata.get("chunk_count", 0) + len(chunks)
                                        metadata["last_updated"] = datetime.now().isoformat()
                                        
                                        # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
                                        if "file_count" not in metadata:
                                            metadata["file_count"] = {"total": 0}
                                        
                                        metadata["file_count"]["total"] = metadata["file_count"].get("total", 0) + 1
                                        
                                        file_ext = os.path.splitext(uploaded_file.name)[1].lower().lstrip('.')
                                        metadata["file_count"][file_ext] = metadata["file_count"].get(file_ext, 0) + 1
                                        
                                        with open(metadata_path, "w", encoding="utf-8") as f:
                                            json.dump(metadata, f, ensure_ascii=False, indent=2)
                                    except Exception as e:
                                        st.warning(f"âš ï¸ æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                                
                                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                os.remove(file_path)
                                
                                st.success(f"âœ… æˆåŠŸæ·»åŠ  {len(chunks)} ä¸ªæ–‡æœ¬å—åˆ°æ•°æ®åº“ '{selected_db}'")
                                
                            except Exception as e:
                                st.error(f"âŒ æ·»åŠ å†…å®¹å¤±è´¥: {str(e)}")
                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                if os.path.exists(file_path):
                                    os.remove(file_path)
            
            elif input_method == "ç›´æ¥è¾“å…¥æ–‡æœ¬":
                text_input = st.text_area(
                    "è¾“å…¥è¦æ·»åŠ çš„æ–‡æœ¬å†…å®¹", 
                    height=300, 
                    key="add_text_input"
                )
                
                if text_input and selected_db:
                    if st.button("æ·»åŠ æ–‡æœ¬åˆ°æ•°æ®åº“", key="add_from_text"):
                        with st.spinner("å¤„ç†ä¸­..."):
                            try:
                                # åˆ†å‰²æ–‡æœ¬
                                text_splitter = RecursiveCharacterTextSplitter(
                                    chunk_size=chunk_size,
                                    chunk_overlap=chunk_overlap
                                )
                                chunks = text_splitter.create_documents([text_input])
                                
                                # è·å–åµŒå…¥å¯¹è±¡
                                try:
                                    embeddings = get_embeddings(
                                        embedding_provider,
                                        api_key if embedding_provider == "openai" else None,
                                        model if embedding_provider == "openai" else None,
                                        api_base if embedding_provider == "openai" else None
                                    )
                                except Exception as e:
                                    st.error(f"âŒ åˆ›å»ºåµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
                                    st.stop()
                                
                                # åŠ è½½ç°æœ‰æ•°æ®åº“
                                db_path = os.path.join(DEFAULT_DB_PATH, selected_db)
                                
                                vector_db = Chroma(
                                    persist_directory=db_path,
                                    embedding_function=embeddings
                                )
                                
                                # æ·»åŠ æ–°æ–‡æ¡£
                                vector_db.add_documents(chunks)
                                vector_db.persist()
                                
                                # æ›´æ–°å…ƒæ•°æ®
                                metadata_path = os.path.join(db_path, "metadata.json")
                                if os.path.exists(metadata_path):
                                    try:
                                        with open(metadata_path, "r", encoding="utf-8") as f:
                                            metadata = json.load(f)
                                            
                                        # æ›´æ–°å…ƒæ•°æ®
                                        metadata["chunk_count"] = metadata.get("chunk_count", 0) + len(chunks)
                                        metadata["last_updated"] = datetime.now().isoformat()
                                        
                                        with open(metadata_path, "w", encoding="utf-8") as f:
                                            json.dump(metadata, f, ensure_ascii=False, indent=2)
                                    except Exception as e:
                                        st.warning(f"âš ï¸ æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                                
                                st.success(f"âœ… æˆåŠŸæ·»åŠ  {len(chunks)} ä¸ªæ–‡æœ¬å—åˆ°æ•°æ®åº“ '{selected_db}'")
                                
                            except Exception as e:
                                st.error(f"âŒ æ·»åŠ å†…å®¹å¤±è´¥: {str(e)}")
    
    # Tab 3: æœç´¢
    with tabs[2]:
        st.header("ğŸ” æœç´¢æ•°æ®åº“")
        
        if not existing_dbs:
            st.info("âš ï¸ ç›®å‰æ²¡æœ‰ç°æœ‰æ•°æ®åº“ï¼Œè¯·å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®åº“")
        else:
            # å•ä¸€æ•°æ®åº“æœç´¢æˆ–å…¨åº“æœç´¢
            search_mode = st.radio("æœç´¢æ¨¡å¼", ["å•ä¸€æ•°æ®åº“", "å¤šæ•°æ®åº“"], key="search_mode")
            
            if search_mode == "å•ä¸€æ•°æ®åº“":
                selected_db = st.selectbox("é€‰æ‹©è¦æœç´¢çš„æ•°æ®åº“", existing_dbs, key="search_db_select")
                
                query = st.text_input("è¾“å…¥æœç´¢æŸ¥è¯¢", key="search_query")
                
                col1, col2 = st.columns(2)
                with col1:
                    limit = st.slider("è¿”å›ç»“æœæ•°é‡", 1, 20, 5, key="search_limit")
                with col2:
                    min_relevance = st.slider("æœ€ä½ç›¸å…³åº¦", 0.0, 1.0, 0.0, 0.05, key="search_min_relevance", 
                                         help="0è¡¨ç¤ºä¸è¿‡æ»¤ï¼Œå€¼è¶Šé«˜è¡¨ç¤ºç»“æœè¶Šç›¸å…³")
                
                if query:
                    if st.button("æœç´¢", key="search_button"):
                        with st.spinner("æœç´¢ä¸­..."):
                            try:
                                # è·å–åµŒå…¥å¯¹è±¡
                                try:
                                    embeddings = get_embeddings(
                                        embedding_provider,
                                        api_key if embedding_provider == "openai" else None,
                                        model if embedding_provider == "openai" else None,
                                        api_base if embedding_provider == "openai" else None
                                    )
                                except Exception as e:
                                    st.error(f"âŒ åˆ›å»ºåµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
                                    st.stop()
                                
                                # åŠ è½½æ•°æ®åº“
                                db_path = os.path.join(DEFAULT_DB_PATH, selected_db)
                                
                                vector_db = Chroma(
                                    persist_directory=db_path,
                                    embedding_function=embeddings
                                )
                                
                                # ä½¿ç”¨MMRæœç´¢ä»¥æé«˜ç»“æœå¤šæ ·æ€§
                                results = vector_db.max_marginal_relevance_search(
                                    query, 
                                    k=limit*2,  # å…ˆæ£€ç´¢æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤
                                    fetch_k=limit*3
                                )
                                
                                # å¦‚æœè®¾ç½®äº†æœ€ä½ç›¸å…³åº¦ï¼Œåˆ™è¿‡æ»¤ç»“æœ
                                if min_relevance > 0 and results:
                                    # ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ä¼°è®¡ç›¸å…³æ€§ï¼ˆå®Œæ•´å®ç°å¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ–¹æ³•ï¼‰
                                    filtered_results = results[:limit]
                                else:
                                    filtered_results = results[:limit]
                                
                                if not filtered_results:
                                    st.warning(f"âš ï¸ åœ¨æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„å†…å®¹")
                                else:
                                    st.success(f"âœ… æ‰¾åˆ° {len(filtered_results)} ä¸ªç›¸å…³ç»“æœ")
                                    
                                    # æ˜¾ç¤ºç»“æœ
                                    for i, doc in enumerate(filtered_results):
                                        with st.expander(f"ç»“æœ {i+1} - ç›¸å…³åº¦: {min_relevance if min_relevance > 0 else 'æœªè®¡ç®—'}"):
                                            # è·å–æ¥æºä¿¡æ¯
                                            source = doc.metadata.get("source", "")
                                            if not source:
                                                source = doc.metadata.get("source_file", "æœªçŸ¥æ¥æº")
                                                
                                            page = doc.metadata.get("page", "")
                                            source_info = f"{source}"
                                            if page:
                                                source_info += f" (ç¬¬{page}é¡µ)"
                                            
                                            st.info(f"**æ¥æº:** {source_info}")
                                            st.markdown(doc.page_content)
                                
                            except Exception as e:
                                st.error(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
            
            else:  # å¤šæ•°æ®åº“æœç´¢
                selected_dbs = st.multiselect("é€‰æ‹©è¦æœç´¢çš„æ•°æ®åº“", existing_dbs, default=existing_dbs, key="multi_search_db_select")
                
                query = st.text_input("è¾“å…¥æœç´¢æŸ¥è¯¢", key="multi_search_query")
                
                limit_per_db = st.slider("æ¯ä¸ªæ•°æ®åº“è¿”å›ç»“æœæ•°é‡", 1, 10, 3, key="multi_search_limit")
                
                if query and selected_dbs:
                    if st.button("æœç´¢", key="multi_search_button"):
                        with st.spinner("æœç´¢ä¸­..."):
                            try:
                                all_results = []
                                
                                # è·å–åµŒå…¥å¯¹è±¡
                                try:
                                    embeddings = get_embeddings(
                                        embedding_provider,
                                        api_key if embedding_provider == "openai" else None,
                                        model if embedding_provider == "openai" else None,
                                        api_base if embedding_provider == "openai" else None
                                    )
                                except Exception as e:
                                    st.error(f"âŒ åˆ›å»ºåµŒå…¥å¯¹è±¡å¤±è´¥: {str(e)}")
                                    st.stop()
                                
                                # åœ¨æ¯ä¸ªæ•°æ®åº“ä¸­æœç´¢
                                for db in selected_dbs:
                                    db_path = os.path.join(DEFAULT_DB_PATH, db)
                                    try:
                                        vector_db = Chroma(
                                            persist_directory=db_path,
                                            embedding_function=embeddings
                                        )
                                        
                                        results = vector_db.similarity_search_with_score(
                                            query,
                                            k=limit_per_db
                                        )
                                        
                                        # æ·»åŠ æ•°æ®åº“ä¿¡æ¯åˆ°ç»“æœä¸­
                                        for doc, score in results:
                                            doc.metadata["database"] = db
                                            # è½¬æ¢ç›¸ä¼¼åº¦åˆ†æ•°ä¸ºç›¸å…³åº¦åˆ†æ•°
                                            relevance = 1.0 - score/2  # ç®€å•è½¬æ¢
                                            all_results.append((doc, relevance))
                                    except Exception as e:
                                        st.warning(f"âš ï¸ åœ¨æ•°æ®åº“ '{db}' ä¸­æœç´¢æ—¶å‡ºé”™: {str(e)}")
                                
                                # æ’åºç»“æœï¼ˆæŒ‰ç›¸å…³åº¦é™åºï¼‰
                                all_results.sort(key=lambda x: x[1], reverse=True)
                                
                                if not all_results:
                                    st.warning(f"âš ï¸ åœ¨æ‰€æœ‰æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„å†…å®¹")
                                else:
                                    st.success(f"âœ… æ‰¾åˆ° {len(all_results)} ä¸ªç›¸å…³ç»“æœ")
                                    
                                    # æ˜¾ç¤ºç»“æœ
                                    for i, (doc, relevance) in enumerate(all_results):
                                        with st.expander(f"ç»“æœ {i+1} - æ¥è‡ªæ•°æ®åº“: {doc.metadata.get('database', 'æœªçŸ¥')} - ç›¸å…³åº¦: {relevance:.2f}"):
                                            # è·å–æ¥æºä¿¡æ¯
                                            source = doc.metadata.get("source", "")
                                            if not source:
                                                source = doc.metadata.get("source_file", "æœªçŸ¥æ¥æº")
                                                
                                            page = doc.metadata.get("page", "")
                                            source_info = f"{source}"
                                            if page:
                                                source_info += f" (ç¬¬{page}é¡µ)"
                                            
                                            st.info(f"**æ¥æº:** {source_info}")
                                            st.markdown(doc.page_content)
                            
                            except Exception as e:
                                st.error(f"âŒ æœç´¢å¤±è´¥: {str(e)}")
    
    # Tab 4: ç®¡ç†æ•°æ®åº“
    with tabs[3]:
        st.header("ğŸ› ï¸ ç®¡ç†æ•°æ®åº“")
        
        if not existing_dbs:
            st.info("âš ï¸ ç›®å‰æ²¡æœ‰ç°æœ‰æ•°æ®åº“ï¼Œè¯·å…ˆåˆ›å»ºä¸€ä¸ªæ•°æ®åº“")
        else:
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.subheader("ç°æœ‰æ•°æ®åº“")
                
                # æ˜¾ç¤ºæ•°æ®åº“åˆ—è¡¨å’Œè¯¦ç»†ä¿¡æ¯
                for db in existing_dbs:
                    with st.expander(f"ğŸ“ {db}"):
                        db_path = os.path.join(DEFAULT_DB_PATH, db)
                        metadata_path = os.path.join(db_path, "metadata.json")
                        
                        if os.path.exists(metadata_path):
                            try:
                                with open(metadata_path, "r", encoding="utf-8") as f:
                                    metadata = json.load(f)
                                
                                # æ˜¾ç¤ºæ•°æ®åº“è¯¦ç»†ä¿¡æ¯
                                st.markdown("### æ•°æ®åº“ä¿¡æ¯")
                                
                                # åˆ›å»ºæ—¶é—´
                                created_at = metadata.get("created_at", "æœªçŸ¥")
                                if created_at != "æœªçŸ¥":
                                    try:
                                        created_datetime = datetime.fromisoformat(created_at)
                                        created_at = created_datetime.strftime("%Y-%m-%d %H:%M:%S")
                                    except:
                                        pass
                                
                                # æœ€åæ›´æ–°æ—¶é—´
                                last_updated = metadata.get("last_updated", "ä¸åˆ›å»ºæ—¶é—´ç›¸åŒ")
                                if last_updated != "ä¸åˆ›å»ºæ—¶é—´ç›¸åŒ":
                                    try:
                                        updated_datetime = datetime.fromisoformat(last_updated)
                                        last_updated = updated_datetime.strftime("%Y-%m-%d %H:%M:%S")
                                    except:
                                        pass
                                
                                # ç”Ÿæˆæ¥æºä¿¡æ¯
                                source_info = ""
                                if "file_name" in metadata:
                                    source_info = f"æ–‡ä»¶: {metadata['file_name']}"
                                elif "directory" in metadata:
                                    source_info = f"ç›®å½•: {metadata['directory']}"
                                    if "file_count" in metadata and isinstance(metadata["file_count"], dict):
                                        file_count = metadata["file_count"]
                                        if "total" in file_count:
                                            source_info += f" ({file_count['total']} ä¸ªæ–‡ä»¶)"
                                elif "source" in metadata:
                                    source_info = f"æ¥æº: {metadata['source']}"
                                
                                # å—ä¿¡æ¯
                                chunk_size = metadata.get("chunk_size", "æœªçŸ¥")
                                chunk_overlap = metadata.get("chunk_overlap", "æœªçŸ¥")
                                chunk_count = metadata.get("chunk_count", "æœªçŸ¥")
                                
                                # åµŒå…¥æ¨¡å‹ä¿¡æ¯
                                embedding_provider = metadata.get("embedding_provider", "æœªçŸ¥")
                                embedding_model = metadata.get("embedding_model", "æœªçŸ¥")
                                
                                # æ˜¾ç¤ºä¿¡æ¯è¡¨æ ¼
                                st.info(f"""
                                - **åç§°:** {db}
                                - **åˆ›å»ºæ—¶é—´:** {created_at}
                                - **æœ€åæ›´æ–°:** {last_updated}
                                - **æ¥æº:** {source_info}
                                - **æ–‡æœ¬å—:** {chunk_count} ä¸ª (å¤§å°: {chunk_size}, é‡å : {chunk_overlap})
                                - **åµŒå…¥æ¨¡å‹:** {embedding_provider} ({embedding_model})
                                """)
                                
                                # æ˜¾ç¤ºæ–‡ä»¶ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
                                if "file_count" in metadata and isinstance(metadata["file_count"], dict):
                                    file_count = metadata["file_count"]
                                    if len(file_count) > 1:  # ä¸ä»…ä»…åªæœ‰totalé”®
                                        st.subheader("æ–‡ä»¶ç»Ÿè®¡")
                                        file_stats = {k: v for k, v in file_count.items() if k != "total"}
                                        st.json(file_stats)
                                
                            except Exception as e:
                                st.error(f"æ— æ³•è¯»å–æ•°æ®åº“å…ƒæ•°æ®: {str(e)}")
                        else:
                            st.warning("âš ï¸ æœªæ‰¾åˆ°æ•°æ®åº“å…ƒæ•°æ®æ–‡ä»¶")
                        
                        # æ•°æ®åº“æ“ä½œæŒ‰é’®
                        col_rename, col_delete = st.columns(2)
                        
                        with col_rename:
                            if st.button("é‡å‘½å", key=f"rename_{db}"):
                                st.session_state[f"rename_db_{db}"] = True
                                st.rerun()
                        
                        with col_delete:
                            if st.button("åˆ é™¤", key=f"delete_{db}"):
                                with st.popover("ç¡®è®¤åˆ é™¤"):
                                    st.warning(f"ç¡®å®šè¦åˆ é™¤æ•°æ®åº“ '{db}' å—ï¼Ÿæ­¤æ“ä½œä¸å¯æ’¤é”€ã€‚")
                                    if st.button("ç¡®è®¤åˆ é™¤", key=f"confirm_delete_{db}"):
                                        try:
                                            shutil.rmtree(db_path)
                                            st.success(f"âœ… æ•°æ®åº“ '{db}' å·²åˆ é™¤")
                                            st.rerun()
                                        except Exception as e:
                                            st.error(f"âŒ åˆ é™¤æ•°æ®åº“å¤±è´¥: {str(e)}")
                        
                        # é‡å‘½åè¡¨å•
                        if f"rename_db_{db}" in st.session_state and st.session_state[f"rename_db_{db}"]:
                            with st.form(key=f"rename_form_{db}"):
                                new_name = st.text_input("æ–°æ•°æ®åº“åç§°", value=db)
                                
                                col_submit, col_cancel = st.columns(2)
                                with col_submit:
                                    submit = st.form_submit_button("ç¡®è®¤é‡å‘½å")
                                with col_cancel:
                                    cancel = st.form_submit_button("å–æ¶ˆ")
                                
                                if submit and new_name and new_name != db:
                                    try:
                                        new_db_path = os.path.join(DEFAULT_DB_PATH, new_name)
                                        
                                        if os.path.exists(new_db_path):
                                            st.error(f"âŒ æ•°æ®åº“åç§° '{new_name}' å·²å­˜åœ¨")
                                        else:
                                            # é‡å‘½åæ•°æ®åº“ç›®å½•
                                            shutil.move(db_path, new_db_path)
                                            
                                            # æ›´æ–°å…ƒæ•°æ®
                                            metadata_path = os.path.join(new_db_path, "metadata.json")
                                            if os.path.exists(metadata_path):
                                                try:
                                                    with open(metadata_path, "r", encoding="utf-8") as f:
                                                        metadata = json.load(f)
                                                    
                                                    metadata["name"] = new_name
                                                    metadata["last_updated"] = datetime.now().isoformat()
                                                    
                                                    with open(metadata_path, "w", encoding="utf-8") as f:
                                                        json.dump(metadata, f, ensure_ascii=False, indent=2)
                                                except Exception as e:
                                                    st.warning(f"âš ï¸ æ›´æ–°å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                                            
                                            st.success(f"âœ… æ•°æ®åº“å·²é‡å‘½åä¸º '{new_name}'")
                                            st.session_state.pop(f"rename_db_{db}")
                                            st.rerun()
                                    except Exception as e:
                                        st.error(f"âŒ é‡å‘½åæ•°æ®åº“å¤±è´¥: {str(e)}")
                                
                                if cancel:
                                    st.session_state.pop(f"rename_db_{db}")
                                    st.rerun()
            
            with col2:
                st.subheader("æ‰¹é‡æ“ä½œ")
                
                # å¤‡ä»½æ‰€æœ‰æ•°æ®åº“
                if st.button("å¤‡ä»½æ‰€æœ‰æ•°æ®åº“", key="backup_all"):
                    with st.spinner("åˆ›å»ºå¤‡ä»½ä¸­..."):
                        try:
                            backup_dir = os.path.join(os.path.dirname(DEFAULT_DB_PATH), "backups")
                            os.makedirs(backup_dir, exist_ok=True)
                            
                            # åˆ›å»ºå¤‡ä»½æ–‡ä»¶åï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            backup_file = os.path.join(backup_dir, f"vectordb_backup_{timestamp}.zip")
                            
                            # åˆ›å»ºZIPæ–‡ä»¶
                            with st.progress(0) as progress:
                                # è®¡ç®—è¦å¤„ç†çš„ç›®å½•æ€»æ•°
                                total_dbs = len(existing_dbs)
                                
                                with zipfile.ZipFile(backup_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
                                    for i, db in enumerate(existing_dbs):
                                        db_path = os.path.join(DEFAULT_DB_PATH, db)
                                        
                                        # æ›´æ–°è¿›åº¦
                                        progress.progress((i) / total_dbs)
                                        
                                        # é€’å½’æ·»åŠ æ‰€æœ‰æ–‡ä»¶
                                        for root, _, files in os.walk(db_path):
                                            for file in files:
                                                file_path = os.path.join(root, file)
                                                # è·å–ç›¸å¯¹è·¯å¾„
                                                rel_path = os.path.relpath(file_path, DEFAULT_DB_PATH)
                                                zipf.write(file_path, rel_path)
                                        
                                progress.progress(1.0)
                            
                            st.success(f"âœ… å¤‡ä»½å·²åˆ›å»º: {backup_file}")
                            
                            # æä¾›ä¸‹è½½é“¾æ¥
                            with open(backup_file, "rb") as f:
                                st.download_button(
                                    label="ä¸‹è½½å¤‡ä»½æ–‡ä»¶",
                                    data=f.read(),
                                    file_name=f"vectordb_backup_{timestamp}.zip",
                                    mime="application/zip"
                                )
                            
                        except Exception as e:
                            st.error(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {str(e)}")
                
                # ä¸Šä¼ å¤‡ä»½
                st.subheader("æ¢å¤å¤‡ä»½")
                uploaded_backup = st.file_uploader("ä¸Šä¼ å¤‡ä»½æ–‡ä»¶", type=["zip"], help="ä¸Šä¼ ä¹‹å‰å¯¼å‡ºçš„ZIPå¤‡ä»½æ–‡ä»¶")
                
                if uploaded_backup:
                    if st.button("æ¢å¤å¤‡ä»½", key="restore_backup"):
                        with st.spinner("æ¢å¤å¤‡ä»½ä¸­..."):
                            try:
                                # ä¿å­˜ä¸Šä¼ çš„å¤‡ä»½æ–‡ä»¶
                                backup_path = os.path.join(TEMP_DIR, uploaded_backup.name)
                                
                                with open(backup_path, "wb") as f:
                                    f.write(uploaded_backup.getbuffer())
                                
                                # è§£å‹ç¼©
                                with zipfile.ZipFile(backup_path, 'r') as zipf:
                                    # è·å–ZIPæ–‡ä»¶ä¸­çš„æ‰€æœ‰æ•°æ®åº“åç§°
                                    all_items = zipf.namelist()
                                    db_names = set()
                                    
                                    for item in all_items:
                                        parts = item.split('/')
                                        if len(parts) > 0:
                                            db_names.add(parts[0])
                                    
                                    # è¯¢é—®è¦æ¢å¤çš„æ•°æ®åº“
                                    if db_names:
                                        for db in db_names:
                                            with st.expander(f"æ¢å¤æ•°æ®åº“: {db}"):
                                                restore_mode = st.radio(
                                                    "æ¢å¤æ¨¡å¼",
                                                    ["è¦†ç›–ç°æœ‰æ•°æ®åº“", "åˆ›å»ºæ–°æ•°æ®åº“"],
                                                    key=f"restore_mode_{db}"
                                                )
                                                
                                                new_name = db
                                                if restore_mode == "åˆ›å»ºæ–°æ•°æ®åº“":
                                                    new_name = st.text_input("æ–°æ•°æ®åº“åç§°", 
                                                                       value=f"{db}_restored",
                                                                       key=f"new_name_{db}")
                                                
                                                if st.button("æ¢å¤æ­¤æ•°æ®åº“", key=f"restore_db_{db}"):
                                                    target_path = os.path.join(DEFAULT_DB_PATH, new_name)
                                                    
                                                    # æ£€æŸ¥ç›®æ ‡è·¯å¾„
                                                    if os.path.exists(target_path) and restore_mode == "è¦†ç›–ç°æœ‰æ•°æ®åº“":
                                                        shutil.rmtree(target_path)
                                                    
                                                    os.makedirs(target_path, exist_ok=True)
                                                    
                                                    # è§£å‹æ­¤æ•°æ®åº“çš„æ–‡ä»¶
                                                    for item in all_items:
                                                        if item.startswith(f"{db}/"):
                                                            # ä¿®æ”¹è·¯å¾„ä»¥é€‚åº”æ–°çš„æ•°æ®åº“åç§°
                                                            if restore_mode == "åˆ›å»ºæ–°æ•°æ®åº“":
                                                                new_item = item.replace(f"{db}/", f"{new_name}/", 1)
                                                                extract_path = os.path.join(DEFAULT_DB_PATH, new_item)
                                                            else:
                                                                extract_path = os.path.join(DEFAULT_DB_PATH, item)
                                                            
                                                            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                                                            os.makedirs(os.path.dirname(extract_path), exist_ok=True)
                                                            
                                                            # è§£å‹æ–‡ä»¶
                                                            with zipf.open(item) as source, open(extract_path, 'wb') as target:
                                                                shutil.copyfileobj(source, target)
                                                    
                                                    st.success(f"âœ… æ•°æ®åº“ '{db}' å·²æ¢å¤ä¸º '{new_name}'")
                                    else:
                                        st.error("âŒ å¤‡ä»½æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®åº“")
                                
                                # åˆ é™¤ä¸´æ—¶å¤‡ä»½æ–‡ä»¶
                                os.remove(backup_path)
                                
                            except Exception as e:
                                st.error(f"âŒ æ¢å¤å¤‡ä»½å¤±è´¥: {str(e)}")
                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                if os.path.exists(backup_path):
                                    os.remove(backup_path)

if __name__ == "__main__":
    main() 